name: Sync GitHub Data to Turso

on:
  # Weekly sync every Sunday at 2 AM UTC to minimize writes
  schedule:
    - cron: '0 2 * * 0'
  # Allow manual trigger for testing
  workflow_dispatch:

jobs:
  sync-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
    
    - name: Install Turso CLI
      run: |
        curl -sSfL https://get.tur.so/install.sh | bash
        echo "$HOME/.turso/bin" >> $GITHUB_PATH
        export PATH="$HOME/.turso/bin:$PATH"
    
    - name: Fetch GitHub data and sync to Turso
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
        TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL }}
        GITHUB_USERNAME: ${{ github.repository_owner }}
      run: |
        # Create temporary script for data fetching
        cat << 'EOF' > fetch_github_data.js
        const { Octokit } = require('@octokit/rest')
        const fs = require('fs')

        const octokit = new Octokit({
          auth: process.env.GITHUB_TOKEN
        })

        async function fetchGitHubData() {
          const username = process.env.GITHUB_USERNAME
          console.log(`Fetching data for user: ${username}`)
          
          try {
            // Fetch user repositories (limited to 100 to stay within API limits)
            const { data: repos } = await octokit.rest.repos.listForUser({
              username,
              sort: 'updated',
              per_page: 100
            })

            console.log(`Found ${repos.length} repositories`)

            // Create SQL statements with minimal data to save storage
            let sql = `
            /* Create tables if they don't exist */
            CREATE TABLE IF NOT EXISTS repositories (
              id INTEGER PRIMARY KEY,
              name TEXT NOT NULL,
              full_name TEXT NOT NULL,
              description TEXT,
              language TEXT,
              stars INTEGER DEFAULT 0,
              forks INTEGER DEFAULT 0,
              open_issues INTEGER DEFAULT 0,
              size_kb INTEGER DEFAULT 0,
              created_at TEXT,
              updated_at TEXT,
              pushed_at TEXT,
              is_fork BOOLEAN DEFAULT 0,
              is_private BOOLEAN DEFAULT 0,
              homepage TEXT,
              topics TEXT,
              sync_date TEXT DEFAULT CURRENT_TIMESTAMP
            );

            CREATE TABLE IF NOT EXISTS commits (
              sha TEXT PRIMARY KEY,
              repo_name TEXT NOT NULL,
              author_name TEXT,
              author_email TEXT,
              message TEXT,
              date TEXT,
              additions INTEGER DEFAULT 0,
              deletions INTEGER DEFAULT 0,
              files_changed INTEGER DEFAULT 0,
              sync_date TEXT DEFAULT CURRENT_TIMESTAMP
            );

            CREATE TABLE IF NOT EXISTS languages (
              id INTEGER PRIMARY KEY AUTOINCREMENT,
              repo_name TEXT NOT NULL,
              language TEXT NOT NULL,
              bytes INTEGER DEFAULT 0,
              percentage REAL DEFAULT 0,
              sync_date TEXT DEFAULT CURRENT_TIMESTAMP,
              UNIQUE(repo_name, language)
            );

            /* Clear existing data to prevent duplicates (efficient for small datasets) */
            DELETE FROM repositories;
            DELETE FROM commits; 
            DELETE FROM languages;

            `

            // Process repositories (limit data to stay under 5GB storage)
            for (const repo of repos.slice(0, 50)) { // Limit to top 50 repos
              const topics = repo.topics ? repo.topics.join(',') : ''
              
              sql += `
              INSERT INTO repositories VALUES (
                ${repo.id},
                ${JSON.stringify(repo.name)},
                ${JSON.stringify(repo.full_name)},
                ${JSON.stringify(repo.description || '')},
                ${JSON.stringify(repo.language || '')},
                ${repo.stargazers_count || 0},
                ${repo.forks_count || 0},
                ${repo.open_issues_count || 0},
                ${repo.size || 0},
                ${JSON.stringify(repo.created_at)},
                ${JSON.stringify(repo.updated_at)},
                ${JSON.stringify(repo.pushed_at)},
                ${repo.fork ? 1 : 0},
                ${repo.private ? 1 : 0},
                ${JSON.stringify(repo.homepage || '')},
                ${JSON.stringify(topics)},
                CURRENT_TIMESTAMP
              );
              `

              // Fetch recent commits (last 10 per repo to minimize API calls)
              try {
                const { data: commits } = await octokit.rest.repos.listCommits({
                  owner: username,
                  repo: repo.name,
                  per_page: 10,
                  since: new Date(Date.now() - 30 * 24 * 60 * 60 * 1000).toISOString() // Last 30 days
                })

                for (const commit of commits) {
                  if (commit.author) {
                    sql += `
                    INSERT OR IGNORE INTO commits VALUES (
                      ${JSON.stringify(commit.sha)},
                      ${JSON.stringify(repo.name)},
                      ${JSON.stringify(commit.commit.author.name)},
                      ${JSON.stringify(commit.commit.author.email)},
                      ${JSON.stringify(commit.commit.message.split('\n')[0].substring(0, 200))},
                      ${JSON.stringify(commit.commit.author.date)},
                      0, 0, 0,
                      CURRENT_TIMESTAMP
                    );
                    `
                  }
                }
              } catch (error) {
                console.log(`Skipping commits for ${repo.name}: ${error.message}`)
              }

              // Fetch languages (cached by GitHub, minimal API impact)
              try {
                const { data: languages } = await octokit.rest.repos.listLanguages({
                  owner: username,
                  repo: repo.name
                })

                const totalBytes = Object.values(languages).reduce((sum, bytes) => sum + bytes, 0)
                
                for (const [language, bytes] of Object.entries(languages)) {
                  const percentage = totalBytes > 0 ? (bytes / totalBytes) * 100 : 0
                  
                  sql += `
                  INSERT OR REPLACE INTO languages (repo_name, language, bytes, percentage, sync_date) VALUES (
                    ${JSON.stringify(repo.name)},
                    ${JSON.stringify(language)},
                    ${bytes},
                    ${percentage.toFixed(2)},
                    CURRENT_TIMESTAMP
                  );
                  `
                }
              } catch (error) {
                console.log(`Skipping languages for ${repo.name}: ${error.message}`)
              }

              // Rate limiting: small delay between repos
              await new Promise(resolve => setTimeout(resolve, 100))
            }

            // Write SQL to file
            fs.writeFileSync('github_data.sql', sql)
            console.log('SQL file generated successfully')
            
            // Log estimated operations for monitoring
            const lines = sql.split('\n').filter(line => line.trim().indexOf('INSERT') === 0).length
            console.log(`Estimated write operations: ${lines} (Limit: 10M)`)
            
          } catch (error) {
            console.error('Error fetching GitHub data:', error)
            process.exit(1)
          }
        }

        fetchGitHubData()
        EOF

        # Install required packages
        npm init -y > /dev/null 2>&1
        npm install @octokit/rest > /dev/null 2>&1

        # Run the script
        node fetch_github_data.js

        # Use full path to Turso CLI and upload to Turso
        TURSO_CLI="$HOME/.turso/bin/turso"
        
        # Authenticate with Turso using auth token
        $TURSO_CLI auth api-tokens add temp-token $TURSO_AUTH_TOKEN
        
        echo "Syncing data to Turso database..."
        $TURSO_CLI db shell ai-data-explorer --file github_data.sql
        
        echo "âœ… Data sync completed successfully"
        
        # Cleanup
        rm -f github_data.sql
        rm -f fetch_github_data.js
        rm -f package.json package-lock.json
        rm -rf node_modules