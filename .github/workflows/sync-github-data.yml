name: Sync GitHub Data to Turso

on:
  # Weekly sync every Sunday at 2 AM UTC to minimize writes
  schedule:
    - cron: '0 2 * * 0'
  # Allow manual trigger for testing
  workflow_dispatch:

jobs:
  sync-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
    
    - name: Install required tools
      run: |
        sudo apt-get update
        sudo apt-get install -y jq
    
    - name: Install Turso CLI
      run: |
        curl -sSfL https://get.tur.so/install.sh | bash
        source /home/runner/.bashrc
        echo "Turso CLI installed, checking location:"
        ls -la $HOME/.turso/
        find $HOME/.turso -name "turso" -type f
        echo "$HOME/.turso" >> $GITHUB_PATH
    
    - name: Fetch GitHub data and sync to Turso
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
        TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL }}
        GITHUB_USERNAME: ${{ github.repository_owner }}
      run: |
        # Create temporary script for data fetching
        cat << 'EOF' > fetch_github_data.js
        const { Octokit } = require('@octokit/rest')
        const fs = require('fs')

        const octokit = new Octokit({
          auth: process.env.GITHUB_TOKEN
        })

        async function fetchGitHubData() {
          const username = process.env.GITHUB_USERNAME
          console.log(`Fetching data for user: ${username}`)
          
          try {
            // Fetch user repositories (limited to 100 to stay within API limits)
            const { data: repos } = await octokit.rest.repos.listForUser({
              username,
              sort: 'updated',
              per_page: 100
            })

            console.log(`Found ${repos.length} repositories`)

            // Create SQL statements with minimal data to save storage
            let sql = `
            /* Create tables if they don't exist */
            CREATE TABLE IF NOT EXISTS repositories (
              id INTEGER PRIMARY KEY,
              name TEXT NOT NULL,
              full_name TEXT NOT NULL,
              description TEXT,
              language TEXT,
              stars INTEGER DEFAULT 0,
              forks INTEGER DEFAULT 0,
              open_issues INTEGER DEFAULT 0,
              size_kb INTEGER DEFAULT 0,
              created_at TEXT,
              updated_at TEXT,
              pushed_at TEXT,
              is_fork BOOLEAN DEFAULT 0,
              is_private BOOLEAN DEFAULT 0,
              homepage TEXT,
              topics TEXT,
              sync_date TEXT DEFAULT CURRENT_TIMESTAMP
            );

            CREATE TABLE IF NOT EXISTS commits (
              sha TEXT PRIMARY KEY,
              repo_name TEXT NOT NULL,
              author_name TEXT,
              author_email TEXT,
              message TEXT,
              date TEXT,
              additions INTEGER DEFAULT 0,
              deletions INTEGER DEFAULT 0,
              files_changed INTEGER DEFAULT 0,
              sync_date TEXT DEFAULT CURRENT_TIMESTAMP
            );

            CREATE TABLE IF NOT EXISTS languages (
              id INTEGER PRIMARY KEY AUTOINCREMENT,
              repo_name TEXT NOT NULL,
              language TEXT NOT NULL,
              bytes INTEGER DEFAULT 0,
              percentage REAL DEFAULT 0,
              sync_date TEXT DEFAULT CURRENT_TIMESTAMP,
              UNIQUE(repo_name, language)
            );

            /* Clear existing data to prevent duplicates (efficient for small datasets) */
            DELETE FROM repositories;
            DELETE FROM commits; 
            DELETE FROM languages;

            `

            // Process repositories (limit data to stay under 5GB storage)
            for (const repo of repos.slice(0, 50)) { // Limit to top 50 repos
              const topics = repo.topics ? repo.topics.join(',') : ''
              
              sql += `
              INSERT INTO repositories VALUES (
                ${repo.id},
                ${JSON.stringify(repo.name)},
                ${JSON.stringify(repo.full_name)},
                ${JSON.stringify(repo.description || '')},
                ${JSON.stringify(repo.language || '')},
                ${repo.stargazers_count || 0},
                ${repo.forks_count || 0},
                ${repo.open_issues_count || 0},
                ${repo.size || 0},
                ${JSON.stringify(repo.created_at)},
                ${JSON.stringify(repo.updated_at)},
                ${JSON.stringify(repo.pushed_at)},
                ${repo.fork ? 1 : 0},
                ${repo.private ? 1 : 0},
                ${JSON.stringify(repo.homepage || '')},
                ${JSON.stringify(topics)},
                CURRENT_TIMESTAMP
              );
              `

              // Fetch recent commits (last 10 per repo to minimize API calls)
              try {
                const { data: commits } = await octokit.rest.repos.listCommits({
                  owner: username,
                  repo: repo.name,
                  per_page: 10,
                  since: new Date(Date.now() - 30 * 24 * 60 * 60 * 1000).toISOString() // Last 30 days
                })

                for (const commit of commits) {
                  if (commit.author) {
                    sql += `
                    INSERT OR IGNORE INTO commits VALUES (
                      ${JSON.stringify(commit.sha)},
                      ${JSON.stringify(repo.name)},
                      ${JSON.stringify(commit.commit.author.name)},
                      ${JSON.stringify(commit.commit.author.email)},
                      ${JSON.stringify(commit.commit.message.split('\n')[0].substring(0, 200))},
                      ${JSON.stringify(commit.commit.author.date)},
                      0, 0, 0,
                      CURRENT_TIMESTAMP
                    );
                    `
                  }
                }
              } catch (error) {
                console.log(`Skipping commits for ${repo.name}: ${error.message}`)
              }

              // Fetch languages (cached by GitHub, minimal API impact)
              try {
                const { data: languages } = await octokit.rest.repos.listLanguages({
                  owner: username,
                  repo: repo.name
                })

                const totalBytes = Object.values(languages).reduce((sum, bytes) => sum + bytes, 0)
                
                for (const [language, bytes] of Object.entries(languages)) {
                  const percentage = totalBytes > 0 ? (bytes / totalBytes) * 100 : 0
                  
                  sql += `
                  INSERT OR REPLACE INTO languages (repo_name, language, bytes, percentage, sync_date) VALUES (
                    ${JSON.stringify(repo.name)},
                    ${JSON.stringify(language)},
                    ${bytes},
                    ${percentage.toFixed(2)},
                    CURRENT_TIMESTAMP
                  );
                  `
                }
              } catch (error) {
                console.log(`Skipping languages for ${repo.name}: ${error.message}`)
              }

              // Rate limiting: small delay between repos
              await new Promise(resolve => setTimeout(resolve, 100))
            }

            // Write SQL to file
            fs.writeFileSync('github_data.sql', sql)
            console.log('SQL file generated successfully')
            
            // Log estimated operations for monitoring
            const lines = sql.split('\n').filter(line => line.trim().indexOf('INSERT') === 0).length
            console.log(`Estimated write operations: ${lines} (Limit: 10M)`)
            
          } catch (error) {
            console.error('Error fetching GitHub data:', error)
            process.exit(1)
          }
        }

        fetchGitHubData()
        EOF

        # Install required packages
        npm init -y > /dev/null 2>&1
        npm install @octokit/rest > /dev/null 2>&1

        # Run the script
        node fetch_github_data.js

        # Use Turso HTTP API directly instead of CLI
        echo "Syncing data to Turso database via HTTP API..."
        
        # Split SQL file by semicolons and execute complete statements
        # Disable exit on error for this section to handle individual statement failures
        set +e
        
        cat github_data.sql | tr -d '\n' | sed 's/;/;\n/g' | while read -r statement; do
          if [[ -n "$statement" && "$statement" != "/*"* && "$statement" =~ [A-Za-z] ]]; then
            statement=$(echo "$statement" | xargs)  # trim whitespace
            echo "Executing: ${statement:0:60}..."
            
            # Escape quotes for JSON - more thorough escaping
            escaped_sql=$(echo "$statement" | sed 's/\\/\\\\/g; s/"/\\"/g; s/\t/\\t/g; s/\n/\\n/g; s/\r/\\r/g')
            
            # Create the JSON payload
            json_payload=$(printf '{"requests":[{"type":"execute","stmt":{"sql":"%s"}}]}' "$escaped_sql")
            
            response=$(curl -s -w "\nHTTP_CODE:%{http_code}" -X POST "$TURSO_DATABASE_URL/v2/pipeline" \
              -H "Authorization: Bearer $TURSO_AUTH_TOKEN" \
              -H "Content-Type: application/json" \
              -d "$json_payload" || echo -e '\n{"error": "curl_failed"}\nHTTP_CODE:000')
            
            # Extract HTTP status code and response body
            http_code=$(echo "$response" | grep "HTTP_CODE:" | cut -d: -f2)
            response_body=$(echo "$response" | sed '/HTTP_CODE:/d')
            
            # Check for HTTP errors first
            if [[ "$http_code" != "200" ]]; then
              echo "HTTP Error $http_code: $response_body"
              continue
            fi
            
            # Check for errors (fallback if jq fails)
            if command -v jq >/dev/null 2>&1; then
              if echo "$response_body" | jq -e '.results[0].response.type == "error"' > /dev/null 2>&1; then
                echo "SQL Error: $(echo "$response_body" | jq -r '.results[0].response.error.message' 2>/dev/null || echo "Unknown error")"
                echo "Response: $response_body"
              elif echo "$response_body" | jq -e '.results[0].response.type' > /dev/null 2>&1; then
                response_type=$(echo "$response_body" | jq -r '.results[0].response.type' 2>/dev/null || echo "ok")
                if [[ "$response_type" == "ok" ]]; then
                  affected_rows=$(echo "$response_body" | jq -r '.results[0].response.result.affected_row_count // 0' 2>/dev/null || echo "unknown")
                  echo "Success: $response_type (affected rows: $affected_rows)"
                else
                  echo "Response: $response_type"
                fi
              else
                echo "Success: Response received"
              fi
            else
              # Fallback without jq
              if [[ "$response_body" == *'"type":"error"'* ]]; then
                echo "Error detected: $response_body"
              elif [[ "$response_body" == *'"type":"ok"'* ]]; then
                echo "Success: SQL executed"
              else
                echo "Unknown response: $response_body"
              fi
            fi
          fi
        done
        
        # Re-enable exit on error
        set -e
        
        # Verify data was inserted by checking row counts
        echo "Verifying data insertion..."
        for table in repositories commits languages; do
          count_response=$(curl -s -X POST "$TURSO_DATABASE_URL/v2/pipeline" \
            -H "Authorization: Bearer $TURSO_AUTH_TOKEN" \
            -H "Content-Type: application/json" \
            -d "{\"requests\": [{\"type\": \"execute\", \"stmt\": {\"sql\": \"SELECT COUNT(*) as count FROM $table\"}}]}")
          
          if command -v jq >/dev/null 2>&1; then
            count=$(echo "$count_response" | jq -r '.results[0].response.result.rows[0][0] // 0' 2>/dev/null || echo "unknown")
            echo "$table table: $count rows"
          else
            echo "$table table: response received"
          fi
        done
        
        echo "âœ… Data sync completed successfully"
        
        # Cleanup
        rm -f github_data.sql
        rm -f fetch_github_data.js
        rm -f package.json package-lock.json
        rm -rf node_modules