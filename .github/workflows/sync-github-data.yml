name: Sync GitHub Data to Turso

on:
  # Weekly sync every Sunday at 2 AM UTC to minimize writes
  schedule:
    - cron: '0 2 * * 0'
  # Allow manual trigger for testing
  workflow_dispatch:

jobs:
  sync-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
    
    - name: Install required tools
      run: |
        sudo apt-get update
        sudo apt-get install -y jq
    
    - name: Install Turso CLI
      run: |
        curl -sSfL https://get.tur.so/install.sh | bash
        source /home/runner/.bashrc
        echo "Turso CLI installed, checking location:"
        ls -la $HOME/.turso/
        find $HOME/.turso -name "turso" -type f
        echo "$HOME/.turso" >> $GITHUB_PATH
    
    - name: Fetch GitHub data and sync to Turso
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
        TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL }}
        GITHUB_USERNAME: ${{ github.repository_owner }}
      run: |
        # Create temporary script for data fetching
        cat << 'EOF' > fetch_github_data.js
        const { Octokit } = require('@octokit/rest')
        const fs = require('fs')

        const octokit = new Octokit({
          auth: process.env.GITHUB_TOKEN
        })

        // Helper function to safely quote SQL string values
        function sqlString(value) {
          if (value === null || value === undefined) {
            return 'NULL'
          }
          // Escape single quotes by doubling them
          return "'" + String(value).replace(/'/g, "''") + "'"
        }

        async function fetchGitHubData() {
          const username = process.env.GITHUB_USERNAME
          console.log(`Fetching data for user: ${username}`)
          
          try {
            // Fetch user repositories (limited to 100 to stay within API limits)
            const { data: repos } = await octokit.rest.repos.listForUser({
              username,
              sort: 'updated',
              per_page: 100
            })

            console.log(`Found ${repos.length} repositories`)

            // Create SQL statements with minimal data to save storage
            let sql = `
            /* Create tables if they don't exist */
            CREATE TABLE IF NOT EXISTS repositories (
              id INTEGER PRIMARY KEY,
              name TEXT NOT NULL,
              full_name TEXT NOT NULL,
              description TEXT,
              language TEXT,
              stars INTEGER DEFAULT 0,
              forks INTEGER DEFAULT 0,
              open_issues INTEGER DEFAULT 0,
              size_kb INTEGER DEFAULT 0,
              created_at TEXT,
              updated_at TEXT,
              pushed_at TEXT,
              is_fork BOOLEAN DEFAULT 0,
              is_private BOOLEAN DEFAULT 0,
              homepage TEXT,
              topics TEXT,
              sync_date TEXT DEFAULT CURRENT_TIMESTAMP
            );

            CREATE TABLE IF NOT EXISTS commits (
              sha TEXT PRIMARY KEY,
              repo_name TEXT NOT NULL,
              author_name TEXT,
              author_email TEXT,
              message TEXT,
              date TEXT,
              additions INTEGER DEFAULT 0,
              deletions INTEGER DEFAULT 0,
              files_changed INTEGER DEFAULT 0,
              sync_date TEXT DEFAULT CURRENT_TIMESTAMP
            );

            CREATE TABLE IF NOT EXISTS languages (
              id INTEGER PRIMARY KEY AUTOINCREMENT,
              repo_name TEXT NOT NULL,
              language TEXT NOT NULL,
              bytes INTEGER DEFAULT 0,
              percentage REAL DEFAULT 0,
              sync_date TEXT DEFAULT CURRENT_TIMESTAMP,
              UNIQUE(repo_name, language)
            );

            /* Clear existing data to prevent duplicates (efficient for small datasets) */
            DELETE FROM repositories;
            DELETE FROM commits; 
            DELETE FROM languages;

            `

            // Test the sqlString function
            console.log('Testing sqlString function:')
            console.log('sqlString("test"): ' + sqlString("test"))
            console.log('sqlString("test\'s"): ' + sqlString("test's"))
            console.log('sqlString(null): ' + sqlString(null))

            // Process repositories (limit data to stay under 5GB storage)
            for (const repo of repos.slice(0, 50)) { // Limit to top 50 repos
              const topics = repo.topics ? repo.topics.join(',') : ''
              
              sql += `
              INSERT INTO repositories VALUES (
                ${repo.id},
                ${sqlString(repo.name)},
                ${sqlString(repo.full_name)},
                ${sqlString(repo.description || '')},
                ${sqlString(repo.language || '')},
                ${repo.stargazers_count || 0},
                ${repo.forks_count || 0},
                ${repo.open_issues_count || 0},
                ${repo.size || 0},
                ${sqlString(repo.created_at)},
                ${sqlString(repo.updated_at)},
                ${sqlString(repo.pushed_at)},
                ${repo.fork ? 1 : 0},
                ${repo.private ? 1 : 0},
                ${sqlString(repo.homepage || '')},
                ${sqlString(topics)},
                CURRENT_TIMESTAMP
              );
              `

              // Fetch recent commits (last 10 per repo to minimize API calls)
              try {
                const { data: commits } = await octokit.rest.repos.listCommits({
                  owner: username,
                  repo: repo.name,
                  per_page: 10,
                  since: new Date(Date.now() - 30 * 24 * 60 * 60 * 1000).toISOString() // Last 30 days
                })

                for (const commit of commits) {
                  if (commit.author) {
                    sql += `
                    INSERT OR IGNORE INTO commits VALUES (
                      ${sqlString(commit.sha)},
                      ${sqlString(repo.name)},
                      ${sqlString(commit.commit.author.name)},
                      ${sqlString(commit.commit.author.email)},
                      ${sqlString(commit.commit.message.split('\n')[0].substring(0, 200))},
                      ${sqlString(commit.commit.author.date)},
                      0, 0, 0,
                      CURRENT_TIMESTAMP
                    );
                    `
                  }
                }
              } catch (error) {
                console.log(`Skipping commits for ${repo.name}: ${error.message}`)
              }

              // Fetch languages (cached by GitHub, minimal API impact)
              try {
                const { data: languages } = await octokit.rest.repos.listLanguages({
                  owner: username,
                  repo: repo.name
                })

                const totalBytes = Object.values(languages).reduce((sum, bytes) => sum + bytes, 0)
                
                for (const [language, bytes] of Object.entries(languages)) {
                  const percentage = totalBytes > 0 ? (bytes / totalBytes) * 100 : 0
                  
                  sql += `
                  INSERT OR REPLACE INTO languages (repo_name, language, bytes, percentage, sync_date) VALUES (
                    ${sqlString(repo.name)},
                    ${sqlString(language)},
                    ${bytes},
                    ${percentage.toFixed(2)},
                    CURRENT_TIMESTAMP
                  );
                  `
                }
              } catch (error) {
                console.log(`Skipping languages for ${repo.name}: ${error.message}`)
              }

              // Rate limiting: small delay between repos
              await new Promise(resolve => setTimeout(resolve, 100))
            }

            // Write SQL to file
            fs.writeFileSync('github_data.sql', sql)
            console.log('SQL file generated successfully')
            
            // Show a sample of the generated SQL for debugging
            const sampleLines = sql.split('\n').slice(0, 50).join('\n')
            console.log('Sample SQL generated:')
            console.log(sampleLines)
            
            // Show a specific INSERT statement
            const insertLines = sql.split('\n').filter(line => line.trim().startsWith('INSERT'))
            if (insertLines.length > 0) {
              console.log('First INSERT statement:')
              console.log(insertLines[0])
            }
            
            // Log estimated operations for monitoring
            const lines = sql.split('\n').filter(line => line.trim().indexOf('INSERT') === 0).length
            console.log(`Estimated write operations: ${lines} (Limit: 10M)`)
            
          } catch (error) {
            console.error('Error fetching GitHub data:', error)
            process.exit(1)
          }
        }

        fetchGitHubData()
        EOF

        # Install required packages
        npm init -y > /dev/null 2>&1
        npm install @octokit/rest > /dev/null 2>&1

        # Run the script
        node fetch_github_data.js

        # Use Turso HTTP API directly instead of CLI
        echo "Syncing data to Turso database via HTTP API..."
        
        # Convert libsql:// URL to HTTPS format for HTTP API
        if [[ "$TURSO_DATABASE_URL" == libsql://* ]]; then
          # Extract the host from libsql://hostname format
          libsql_host=$(echo "$TURSO_DATABASE_URL" | sed 's|libsql://||' | cut -d'?' -f1)
          HTTP_API_URL="https://$libsql_host"
          echo "Converted libsql URL to HTTP API format"
        else
          HTTP_API_URL="$TURSO_DATABASE_URL"
        fi
        
        # Debug URL and connectivity
        echo "Debug information:"
        echo "Original URL: ${TURSO_DATABASE_URL:0:50}..."
        echo "HTTP API URL: ${HTTP_API_URL:0:50}..."
        echo "Auth token present: $(if [[ -n "$TURSO_AUTH_TOKEN" ]]; then echo "Yes"; else echo "No"; fi)"
        
        # Test basic connectivity to the host
        host_url=$(echo "$HTTP_API_URL" | sed 's|https://||' | cut -d'/' -f1)
        echo "Testing basic connectivity to: $host_url"
        if curl -s --connect-timeout 10 --max-time 15 -o /dev/null -w "HTTP_CODE:%{http_code}" "$HTTP_API_URL" | grep -q "HTTP_CODE:[23]"; then
          echo "✅ Basic connectivity successful"
        else
          echo "❌ Basic connectivity failed - check URL format"
        fi
        
        # Test the database connection
        echo "Testing Turso database connection..."
        test_response=$(curl -s -w "\nHTTP_CODE:%{http_code}" --connect-timeout 10 --max-time 30 -X POST "$HTTP_API_URL/v2/pipeline" \
          -H "Authorization: Bearer $TURSO_AUTH_TOKEN" \
          -H "Content-Type: application/json" \
          -d '{"requests":[{"type":"execute","stmt":{"sql":"SELECT 1"}}]}' || echo -e '\n{"error": "connection_test_failed"}\nHTTP_CODE:000')
        
        test_http_code=$(echo "$test_response" | grep "HTTP_CODE:" | cut -d: -f2)
        test_body=$(echo "$test_response" | sed '/HTTP_CODE:/d')
        
        if [[ "$test_http_code" != "200" ]]; then
          echo "❌ Connection test failed with HTTP $test_http_code"
          echo "HTTP API URL: $HTTP_API_URL"
          echo "Response: $test_body"
          echo "Skipping data sync due to connection issues."
          exit 1
        else
          echo "✅ Database connection successful"
          # Update the API URL for use in the main loop
          API_ENDPOINT="$HTTP_API_URL/v2/pipeline"
        fi
        
        # Split SQL file by semicolons and execute complete statements
        # Disable exit on error for this section to handle individual statement failures
        set +e
        
        cat github_data.sql | tr -d '\n' | sed 's/;/;\n/g' | while read -r statement; do
          if [[ -n "$statement" && "$statement" != "/*"* && "$statement" =~ [A-Za-z] ]]; then
            statement=$(echo "$statement" | xargs)  # trim whitespace
            echo "Executing: ${statement:0:60}..."
            
            # Escape quotes for JSON - more thorough escaping
            escaped_sql=$(echo "$statement" | sed 's/\\/\\\\/g; s/"/\\"/g; s/\t/\\t/g; s/\n/\\n/g; s/\r/\\r/g')
            
            # Create the JSON payload
            json_payload=$(printf '{"requests":[{"type":"execute","stmt":{"sql":"%s"}}]}' "$escaped_sql")
            
            response=$(curl -s -w "\nHTTP_CODE:%{http_code}" --max-time 30 -X POST "$API_ENDPOINT" \
              -H "Authorization: Bearer $TURSO_AUTH_TOKEN" \
              -H "Content-Type: application/json" \
              -d "$json_payload" 2>/dev/null || echo -e '\n{"error": "curl_failed"}\nHTTP_CODE:000')
            
            # Extract HTTP status code and response body
            http_code=$(echo "$response" | grep "HTTP_CODE:" | cut -d: -f2)
            response_body=$(echo "$response" | sed '/HTTP_CODE:/d')
            
            # Check for HTTP errors first
            if [[ "$http_code" != "200" ]]; then
              echo "HTTP Error $http_code: $response_body"
              continue
            fi
            
            # Check for errors and show detailed response info
            if command -v jq >/dev/null 2>&1; then
              if echo "$response_body" | jq -e '.results[0].type == "error"' > /dev/null 2>&1; then
                echo "SQL Error: $(echo "$response_body" | jq -r '.results[0].error.message' 2>/dev/null || echo "Unknown error")"
                echo "Statement: ${statement:0:100}..."
                continue  # Skip to next statement
              elif echo "$response_body" | jq -e '.results[0].response.type' > /dev/null 2>&1; then
                response_type=$(echo "$response_body" | jq -r '.results[0].response.type' 2>/dev/null || echo "ok")
                if [[ "$response_type" == "ok" ]]; then
                  # Try different paths for affected row count
                  affected_rows=$(echo "$response_body" | jq -r '
                    .results[0].response.result.affected_row_count // 
                    .results[0].response.result.changes //
                    .results[0].response.result.affected_rows //
                    "not_available"
                  ' 2>/dev/null || echo "unknown")
                  
                  if [[ "$affected_rows" == "not_available" || "$affected_rows" == "unknown" ]]; then
                    # If we can't get affected rows, check if there's any result data
                    has_result=$(echo "$response_body" | jq -r '.results[0].response.result != null' 2>/dev/null || echo "false")
                    echo "Success: $response_type (result present: $has_result)"
                    echo "Debug - Full response: $response_body"
                  else
                    echo "Success: $response_type (affected rows: $affected_rows)"
                  fi
                else
                  echo "Response: $response_type"
                  echo "Debug - Full response: $response_body"
                fi
              else
                echo "Success: Response received"
                echo "Debug - Full response: $response_body"
              fi
            else
              # Fallback without jq
              if [[ "$response_body" == *'"type":"error"'* ]]; then
                echo "Error detected: $response_body"
              elif [[ "$response_body" == *'"type":"ok"'* ]]; then
                echo "Success: SQL executed"
              else
                echo "Unknown response: $response_body"
              fi
            fi
          fi
        done
        
        # Re-enable exit on error
        set -e
        
        # Verify data was inserted by checking row counts
        echo "Verifying data insertion..."
        set +e  # Allow verification failures without stopping the workflow
        
        for table in repositories commits languages; do
          count_response=$(curl -s --max-time 15 -X POST "$API_ENDPOINT" \
            -H "Authorization: Bearer $TURSO_AUTH_TOKEN" \
            -H "Content-Type: application/json" \
            -d "{\"requests\": [{\"type\": \"execute\", \"stmt\": {\"sql\": \"SELECT COUNT(*) as count FROM $table\"}}]}" 2>/dev/null)
          
          if [[ $? -eq 0 ]] && command -v jq >/dev/null 2>&1; then
            # Handle both string and object responses from Turso
            count=$(echo "$count_response" | jq -r '
              if .results[0].response.result.rows[0][0] then
                if (.results[0].response.result.rows[0][0] | type) == "object" then
                  .results[0].response.result.rows[0][0].value
                else
                  .results[0].response.result.rows[0][0]
                end
              else
                0
              end
            ' 2>/dev/null || echo "unknown")
            echo "$table table: $count rows"
          else
            echo "$table table: verification failed (connection issues)"
          fi
        done
        
        set -e  # Re-enable exit on error
        
        echo "✅ Data sync completed successfully"
        
        # Cleanup
        rm -f github_data.sql
        rm -f fetch_github_data.js
        rm -f package.json package-lock.json
        rm -rf node_modules