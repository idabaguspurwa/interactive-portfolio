name: Sync GitHub Data to Turso

on:
  # Weekly sync every Sunday at 2 AM UTC to minimize writes
  schedule:
    - cron: '0 2 * * 0'
  # Allow manual trigger for testing
  workflow_dispatch:

jobs:
  sync-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
    
    - name: Install required tools
      run: |
        sudo apt-get update
        sudo apt-get install -y jq
    
    - name: Install Turso CLI
      run: |
        curl -sSfL https://get.tur.so/install.sh | bash
        source /home/runner/.bashrc
        echo "Turso CLI installed, checking location:"
        ls -la $HOME/.turso/
        find $HOME/.turso -name "turso" -type f
        echo "$HOME/.turso" >> $GITHUB_PATH
    
    - name: Fetch GitHub data and sync to Turso
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
        TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL }}
        GITHUB_USERNAME: ${{ github.repository_owner }}
      run: |
        # Create temporary script for data fetching
        cat << 'EOF' > fetch_github_data.js
        const { Octokit } = require('@octokit/rest')
        const fs = require('fs')

        const octokit = new Octokit({
          auth: process.env.GITHUB_TOKEN
        })

        // Helper function to safely quote SQL string values
        function sqlString(value) {
          if (value === null || value === undefined) {
            return 'NULL'
          }
          // Escape single quotes by doubling them
          return "'" + String(value).replace(/'/g, "''") + "'"
        }

        async function fetchGitHubData() {
          const username = process.env.GITHUB_USERNAME
          console.log(`Fetching data for user: ${username}`)
          
          try {
            // Fetch user repositories (limited to 100 to stay within API limits)
            const { data: repos } = await octokit.rest.repos.listForUser({
              username,
              sort: 'updated',
              per_page: 100
            })

            console.log(`Found ${repos.length} repositories`)

            // Create SQL statements with minimal data to save storage
            let sql = `
            /* Create tables if they don't exist */
            CREATE TABLE IF NOT EXISTS repositories (
              id INTEGER PRIMARY KEY,
              name TEXT NOT NULL,
              full_name TEXT NOT NULL,
              description TEXT,
              language TEXT,
              stars INTEGER DEFAULT 0,
              forks INTEGER DEFAULT 0,
              open_issues INTEGER DEFAULT 0,
              size_kb INTEGER DEFAULT 0,
              created_at TEXT,
              updated_at TEXT,
              pushed_at TEXT,
              is_fork BOOLEAN DEFAULT 0,
              is_private BOOLEAN DEFAULT 0,
              homepage TEXT,
              topics TEXT,
              sync_date TEXT DEFAULT CURRENT_TIMESTAMP
            );

            CREATE TABLE IF NOT EXISTS commits (
              sha TEXT PRIMARY KEY,
              repo_name TEXT NOT NULL,
              author_name TEXT,
              author_email TEXT,
              message TEXT,
              date TEXT,
              additions INTEGER DEFAULT 0,
              deletions INTEGER DEFAULT 0,
              files_changed INTEGER DEFAULT 0,
              sync_date TEXT DEFAULT CURRENT_TIMESTAMP
            );

            CREATE TABLE IF NOT EXISTS languages (
              id INTEGER PRIMARY KEY AUTOINCREMENT,
              repo_name TEXT NOT NULL,
              language TEXT NOT NULL,
              bytes INTEGER DEFAULT 0,
              percentage REAL DEFAULT 0,
              sync_date TEXT DEFAULT CURRENT_TIMESTAMP,
              UNIQUE(repo_name, language)
            );

            /* Clear existing data to prevent duplicates (efficient for small datasets) */
            DELETE FROM repositories;
            DELETE FROM commits; 
            DELETE FROM languages;

            `

            // Test the sqlString function
            console.log('Testing sqlString function:')
            console.log('sqlString("test"): ' + sqlString("test"))
            console.log('sqlString("test\'s"): ' + sqlString("test's"))
            console.log('sqlString(null): ' + sqlString(null))

            // Process repositories (limit data to stay under 5GB storage)
            for (const repo of repos.slice(0, 50)) { // Limit to top 50 repos
              const topics = repo.topics ? repo.topics.join(',') : ''
              
              // Build INSERT statement with proper string concatenation
              const repoInsert = 'INSERT INTO repositories VALUES (' +
                repo.id + ', ' +
                sqlString(repo.name) + ', ' +
                sqlString(repo.full_name) + ', ' +
                sqlString(repo.description || '') + ', ' +
                sqlString(repo.language || '') + ', ' +
                (repo.stargazers_count || 0) + ', ' +
                (repo.forks_count || 0) + ', ' +
                (repo.open_issues_count || 0) + ', ' +
                (repo.size || 0) + ', ' +
                sqlString(repo.created_at) + ', ' +
                sqlString(repo.updated_at) + ', ' +
                sqlString(repo.pushed_at) + ', ' +
                (repo.fork ? 1 : 0) + ', ' +
                (repo.private ? 1 : 0) + ', ' +
                sqlString(repo.homepage || '') + ', ' +
                sqlString(topics) + ', ' +
                'CURRENT_TIMESTAMP' +
                ');\n'
              
              sql += repoInsert

              // Fetch recent commits (last 10 per repo to minimize API calls)
              try {
                const { data: commits } = await octokit.rest.repos.listCommits({
                  owner: username,
                  repo: repo.name,
                  per_page: 10,
                  since: new Date(Date.now() - 30 * 24 * 60 * 60 * 1000).toISOString() // Last 30 days
                })

                for (const commit of commits) {
                  if (commit.author) {
                    const commitInsert = 'INSERT OR IGNORE INTO commits VALUES (' +
                      sqlString(commit.sha) + ', ' +
                      sqlString(repo.name) + ', ' +
                      sqlString(commit.commit.author.name) + ', ' +
                      sqlString(commit.commit.author.email) + ', ' +
                      sqlString(commit.commit.message.split('\n')[0].substring(0, 200)) + ', ' +
                      sqlString(commit.commit.author.date) + ', ' +
                      '0, 0, 0, ' +
                      'CURRENT_TIMESTAMP' +
                      ');\n'
                    
                    sql += commitInsert
                  }
                }
              } catch (error) {
                console.log(`Skipping commits for ${repo.name}: ${error.message}`)
              }

              // Fetch languages (cached by GitHub, minimal API impact)
              try {
                const { data: languages } = await octokit.rest.repos.listLanguages({
                  owner: username,
                  repo: repo.name
                })

                const totalBytes = Object.values(languages).reduce((sum, bytes) => sum + bytes, 0)
                
                for (const [language, bytes] of Object.entries(languages)) {
                  const percentage = totalBytes > 0 ? (bytes / totalBytes) * 100 : 0
                  
                  const langInsert = 'INSERT OR REPLACE INTO languages (repo_name, language, bytes, percentage, sync_date) VALUES (' +
                    sqlString(repo.name) + ', ' +
                    sqlString(language) + ', ' +
                    bytes + ', ' +
                    percentage.toFixed(2) + ', ' +
                    'CURRENT_TIMESTAMP' +
                    ');\n'
                  
                  sql += langInsert
                }
              } catch (error) {
                console.log(`Skipping languages for ${repo.name}: ${error.message}`)
              }

              // Rate limiting: small delay between repos
              await new Promise(resolve => setTimeout(resolve, 100))
            }

            // Write SQL to file
            fs.writeFileSync('github_data.sql', sql)
            console.log('SQL file generated successfully')
            
            // Show a sample of the generated SQL for debugging
            const sampleLines = sql.split('\n').slice(0, 50).join('\n')
            console.log('Sample SQL generated:')
            console.log(sampleLines)
            
            // Show a specific INSERT statement
            const insertLines = sql.split('\n').filter(line => line.trim().startsWith('INSERT'))
            if (insertLines.length > 0) {
              console.log('First INSERT statement:')
              console.log(insertLines[0])
            }
            
            // Log estimated operations for monitoring
            const lines = sql.split('\n').filter(line => line.trim().indexOf('INSERT') === 0).length
            console.log(`Estimated write operations: ${lines} (Limit: 10M)`)
            
          } catch (error) {
            console.error('Error fetching GitHub data:', error)
            process.exit(1)
          }
        }

        fetchGitHubData()
        EOF

        # Install required packages
        npm init -y > /dev/null 2>&1
        npm install @octokit/rest > /dev/null 2>&1

        # Run the script
        node fetch_github_data.js

        # Use Turso HTTP API directly instead of CLI
        echo "Syncing data to Turso database via HTTP API..."
        
        # Convert libsql:// URL to HTTPS format for HTTP API
        if [[ "$TURSO_DATABASE_URL" == libsql://* ]]; then
          # Extract the host from libsql://hostname format
          libsql_host=$(echo "$TURSO_DATABASE_URL" | sed 's|libsql://||' | cut -d'?' -f1)
          HTTP_API_URL="https://$libsql_host"
          echo "Converted libsql URL to HTTP API format"
        else
          HTTP_API_URL="$TURSO_DATABASE_URL"
        fi
        
        # Debug URL and connectivity
        echo "Debug information:"
        echo "Original URL: ${TURSO_DATABASE_URL:0:50}..."
        echo "HTTP API URL: ${HTTP_API_URL:0:50}..."
        echo "Auth token present: $(if [[ -n "$TURSO_AUTH_TOKEN" ]]; then echo "Yes"; else echo "No"; fi)"
        
        # Test basic connectivity to the host
        host_url=$(echo "$HTTP_API_URL" | sed 's|https://||' | cut -d'/' -f1)
        echo "Testing basic connectivity to: $host_url"
        if curl -s --connect-timeout 10 --max-time 15 -o /dev/null -w "HTTP_CODE:%{http_code}" "$HTTP_API_URL" | grep -q "HTTP_CODE:[23]"; then
          echo "✅ Basic connectivity successful"
        else
          echo "❌ Basic connectivity failed - check URL format"
        fi
        
        # Test the database connection
        echo "Testing Turso database connection..."
        test_response=$(curl -s -w "\nHTTP_CODE:%{http_code}" --connect-timeout 10 --max-time 30 -X POST "$HTTP_API_URL/v2/pipeline" \
          -H "Authorization: Bearer $TURSO_AUTH_TOKEN" \
          -H "Content-Type: application/json" \
          -d '{"requests":[{"type":"execute","stmt":{"sql":"SELECT 1"}}]}' || echo -e '\n{"error": "connection_test_failed"}\nHTTP_CODE:000')
        
        test_http_code=$(echo "$test_response" | grep "HTTP_CODE:" | cut -d: -f2)
        test_body=$(echo "$test_response" | sed '/HTTP_CODE:/d')
        
        if [[ "$test_http_code" != "200" ]]; then
          echo "❌ Connection test failed with HTTP $test_http_code"
          echo "HTTP API URL: $HTTP_API_URL"
          echo "Response: $test_body"
          echo "Skipping data sync due to connection issues."
          exit 1
        else
          echo "✅ Database connection successful"
          # Update the API URL for use in the main loop
          API_ENDPOINT="$HTTP_API_URL/v2/pipeline"
        fi
        
        # Split SQL file by semicolons and execute complete statements
        # Disable exit on error for this section to handle individual statement failures
        set +e
        
        # Use python to properly split SQL statements and execute them one by one
        python3 -c "
        import re
        import subprocess
        import json

        def execute_sql(statement, api_endpoint, auth_token):
            # Clean up the statement
            stmt = statement.strip()
            if not stmt or stmt.startswith('/*') or not any(c.isalpha() for c in stmt):
                return
            
            # Debug: show statement type and first line
            stmt_type = 'UNKNOWN'
            if stmt.upper().startswith('CREATE'):
                stmt_type = 'CREATE'
            elif stmt.upper().startswith('INSERT'):
                stmt_type = 'INSERT'
            elif stmt.upper().startswith('DELETE'):
                stmt_type = 'DELETE'
            
            print(f'Executing {stmt_type}: {stmt[:60]}...')
            
            # Escape for JSON - handle all control characters properly
            escaped_sql = json.dumps(stmt)[1:-1]  # Use json.dumps and remove outer quotes
            
            # Create JSON payload
            payload = f'{{\"requests\":[{{\"type\":\"execute\",\"stmt\":{{\"sql\":\"{escaped_sql}\"}}}}]}}'
            
            # Execute curl command
            cmd = [
                'curl', '-s', '-w', '\nHTTP_CODE:%{http_code}', '--max-time', '30',
                '-X', 'POST', api_endpoint,
                '-H', f'Authorization: Bearer {auth_token}',
                '-H', 'Content-Type: application/json',
                '-d', payload
            ]
            
            try:
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=35)
                response = result.stdout
                
                # Extract HTTP code and body
                lines = response.split('\n')
                http_line = [line for line in lines if line.startswith('HTTP_CODE:')]
                http_code = http_line[0].split(':')[1] if http_line else '000'
                response_body = '\n'.join([line for line in lines if not line.startswith('HTTP_CODE:')])
                
                if http_code != '200':
                    print(f'HTTP Error {http_code}: {response_body}')
                    return
                
                # Parse response
                try:
                    resp_data = json.loads(response_body)
                    if resp_data.get('results', [{}])[0].get('type') == 'error':
                        error_msg = resp_data['results'][0].get('error', {}).get('message', 'Unknown error')
                        print(f'SQL Error: {error_msg}')
                        print(f'Statement: {stmt[:100]}...')
                    else:
                        result_type = resp_data.get('results', [{}])[0].get('response', {}).get('type', 'ok')
                        if result_type == 'execute':
                            affected_rows = resp_data.get('results', [{}])[0].get('response', {}).get('result', {}).get('affected_row_count', 'unknown')
                            print(f'Success: {result_type} (affected rows: {affected_rows})')
                        else:
                            print(f'Response: {result_type}')
                except json.JSONDecodeError:
                    print('Success: Response received')
                    
            except subprocess.TimeoutExpired:
                print('Error: Request timeout')
            except Exception as e:
                print(f'Error: {str(e)}')

        # Read and process SQL file
        with open('github_data.sql', 'r') as f:
            content = f.read()

        # Split by semicolon at end of line, but preserve multi-line statements
        # Use a more robust approach to find statement boundaries
        statements = []
        current_stmt = ''
        in_multiline_comment = False
        
        for line in content.split('\n'):
            line = line.strip()
            
            # Skip empty lines and comments
            if not line or line.startswith('--'):
                continue
                
            # Handle multi-line comments
            if line.startswith('/*'):
                in_multiline_comment = True
            if '*/' in line:
                in_multiline_comment = False
                continue
            if in_multiline_comment:
                continue
            
            # Add line to current statement
            if current_stmt:
                current_stmt += ' ' + line
            else:
                current_stmt = line
            
            # Check if statement ends with semicolon
            if line.endswith(';'):
                if current_stmt.strip():
                    statements.append(current_stmt)
                current_stmt = ''
        
        # Add any remaining statement
        if current_stmt.strip():
            statements.append(current_stmt)
        
        # Debug: count statement types
        create_count = sum(1 for s in statements if s.strip().upper().startswith('CREATE'))
        insert_count = sum(1 for s in statements if s.strip().upper().startswith('INSERT'))
        delete_count = sum(1 for s in statements if s.strip().upper().startswith('DELETE'))
        print(f'Found {len(statements)} total statements: {create_count} CREATE, {insert_count} INSERT, {delete_count} DELETE')

        api_endpoint = '$API_ENDPOINT'
        auth_token = '$TURSO_AUTH_TOKEN'

        # Process CREATE statements first to ensure tables exist
        for stmt in statements:
            if stmt.strip().upper().startswith('CREATE'):
                execute_sql(stmt, api_endpoint, auth_token)
        
        # Then process DELETE statements
        for stmt in statements:
            if stmt.strip().upper().startswith('DELETE'):
                execute_sql(stmt, api_endpoint, auth_token)
        
        # Finally process INSERT statements
        for stmt in statements:
            if stmt.strip().upper().startswith('INSERT'):
                execute_sql(stmt, api_endpoint, auth_token)
        "
        
        # Re-enable exit on error
        set -e
        
        # Verify data was inserted by checking row counts
        echo "Verifying data insertion..."
        set +e  # Allow verification failures without stopping the workflow
        
        for table in repositories commits languages; do
          count_response=$(curl -s --max-time 15 -X POST "$API_ENDPOINT" \
            -H "Authorization: Bearer $TURSO_AUTH_TOKEN" \
            -H "Content-Type: application/json" \
            -d "{\"requests\": [{\"type\": \"execute\", \"stmt\": {\"sql\": \"SELECT COUNT(*) as count FROM $table\"}}]}" 2>/dev/null)
          
          if [[ $? -eq 0 ]] && command -v jq >/dev/null 2>&1; then
            # Handle both string and object responses from Turso
            count=$(echo "$count_response" | jq -r '
              if .results[0].response.result.rows[0][0] then
                if (.results[0].response.result.rows[0][0] | type) == "object" then
                  .results[0].response.result.rows[0][0].value
                else
                  .results[0].response.result.rows[0][0]
                end
              else
                0
              end
            ' 2>/dev/null || echo "unknown")
            echo "$table table: $count rows"
          else
            echo "$table table: verification failed (connection issues)"
          fi
        done
        
        set -e  # Re-enable exit on error
        
        echo "✅ Data sync completed successfully"
        
        # Cleanup
        rm -f github_data.sql
        rm -f fetch_github_data.js
        rm -f package.json package-lock.json
        rm -rf node_modules